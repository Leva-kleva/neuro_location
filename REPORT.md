# Задание 1А

В файле _task_1a.py_ содержится решение задачи.

Решение состоит из следующих этапов:
- Первым делом необходимо подготовить датасет: создаем столбик 1-РФ, 0-не РФ
- Разделяем датасет на выборку train, val, test как 70/10/20
- Создаем и обучаем модель
- Производим оценку модели

В качестве модели взята обычная полносвязная сеть, состоящая из 16 нейронов.
Определены параметры обучения (compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])).

В качестве оценки, чтобы было проще, взят roc_auc_score из sklearn.metrics

Итого:
    
    Результат: 0.963073198312481

Запуск:

    $ python task_1a.py


# Задание 1Б

Решение аналогично заданию 1А, но проведены следующие оптимизации:
- Проведена нормализация входных данных (preprocessing.StandardScaler())
- Оптимизирована функция ошибок (Оптимизация optimizers.SGD(learning_rate=0.5, momentum=0.9, nesterov=True))
- Изменена метрика (metrics=[tf.keras.metrics.AUC()])

Параметры обучения остались те же.

Все изменения кода можно посмотреть так:

    $ vimdiff task_1b.py task_1a.py

Итого:
    
    Результат: 0.9863722690172086

Запуск:

    $ python task_1b.py

# Задание 2
В нейросети из Задания 1Б выполняется 16 умножений 

# Задание 3

За основу взято решение из Задание 1А. Методом перебора удалось получить, 
что при 8 нейронах в скрытом слое мы получаем целевое значение p>0.96. 
Но стоит отметить, что такое поведение нестабильно и периодически
нейросеть дает более худший результат.


Запуск:

    $ python task_1a_opt.py
